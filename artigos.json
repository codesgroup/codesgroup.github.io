[
  {
    "title": "Selecting and combining complementary feature representations and classifiers for hate speech detection",
    "authors": "Rafael M.O. Cruz, Woshington V. de Sousa, George D.C. Cavalcanti",
    "id": 1,
    "text": "Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS.\n\n. ",
    "date": "2021",
    "link": "https://doi.org/10.1016/j.osnem.2021.100194",
    "revista": "Online Social Networks and Media",
    "introduction":"Social networks are an essential communication channel with an exponential increase in the last years. It is considered relevant in several contexts because it allows users to express opinions, share content, and disseminate news. They impact millions of users every day and change the general socialization perspective [1].\n\nSocial media allows users to register quickly and propagate content from the most diverse subjects [2]. Due to a large amount of content published daily on these platforms, the task of monitoring and control its contents is arduous. The lack of proper moderation provides freedom for users to perform cyberbullying or other forms of attacks. The dissemination of hate speech becomes very common in this scenario since anybody with an Internet connection can become a disseminating agent. Users can also use creating fake profiles to avoid identification, thus becoming more confident that they will not be punished. Another problem is bots, which are programmed algorithms that behave like humans in performing tasks like disseminating news and information. Therefore, it can also act as disseminating hate speech agent [3]. According to the data provided by Twitter, approximately 8.5% of users in the platform are bots [4].\n\nHate speech often appears in incidents with a tremendous social repercussion such as the American presidential election, Brexit [5], terrorist attacks associated with Islamic groups [6] and the COVID-19 pandemic, that provoked discrimination against Chinese and Asians [7].\n\nDetecting hate speech is a complex task because it can be easily mistaken with offensive language and humor posts that, in many situations, are protected by laws of freedom of speech, as the Brazilian Civil Rights Framework for the Internet, which was established in 2014. Hate speech is usually defined as any sort of speech targeting a particular group, especially based on race, religion, or sexual orientation. Davidson et al. [8] defined hate speech as: “Language that is used to express hatred towards a targeted group or intended to be derogatory, to humiliate, or to insult the members of the group”. In contrast, offensive language is characterized by the use of terms that are highly offensive to some groups, such as n*gga, and f*g, which are prevalent on social media. However, the text itself is not targeting or inciting violence to a specific group. Fig. 1 illustrates two examples of hateful and two offensive tweets extracted from the Thomas Davidson dataset [9]. This nuance makes human moderation a slow and challenging process. Hence, impossible to deal with a large amount of content created daily.\n\nIn this paper, we explore the problem of automatic hate speech detection in English posts from social media. Two crucial aspects of creating a machine learning solution are extracting features from the text and the classification algorithm employed [10]. There are several feature extraction techniques such as Bag of Word approaches [11], lexical resources [12], sentiment analysis [13] and dense word vectors [14], [15]. The classification algorithm can be classical methods (e.g., Logistic Regression and Naïve Bayes) or Deep Learning approaches like Convolutional Neural Networks (CNN) [16]. Multiple works in hate speech detection are now using an ensemble model to achieve higher performance. However, comparative studies showing which are the most effective feature extraction techniques and classification methods for this problem is still an open research question [10]. Furthermore, the field lacks a methodology to analyze multiple feature representation techniques and classification algorithms."
  },
  {
    "title": "Fake news detection: Taxonomy and comparative study",
    "authors": "Faramarz Farhangian, Rafael M.O. Cruz, George D.C. Cavalcanti",
    "id": 2,
    "text": "The proliferation of social networks has presented a significant challenge in combating the pervasive issue of fake news within modern societies. Due to the large amount of information and news produced daily in text, audio, and video, the validation and verification of this information have become crucial tasks. Leveraging advancements in artificial intelligence, distinguishing between fake news and factual information through automatic fake news detection systems has become more feasible. Automatic fake news detection has been explored from diverse perspectives, employing various feature extraction and classification models. Nonetheless, empirical evaluations, categorization, and comparisons of existing techniques for handling this problem remain limited. In this paper, we revisit the definitions and perspectives of fake news and propose an updated taxonomy for the field based on multiple criteria: (1) Type of features used in fake news detection; (2) Fake news detection perspectives; (3) Feature representation methods; and (4) Classification approaches. Moreover, we conduct an extensive empirical study to evaluate several feature representation techniques and classification approaches based on accuracy and computational cost. Our experimental results demonstrate that the optimal feature extraction techniques vary depending on the characteristics of the dataset. Notably, context-dependent models based on transformer models consistently exhibit superior performance. Additionally, employing transformer models as feature extraction methods, rather than solely fine-tuning the network for the downstream task, improves overall performance. Through extensive error analysis, we identify that a combination of feature representation methods and classification algorithms, including classical ones, offer complementary aspects and should be considered for achieving better generalization performance while maintaining a relatively low computational cost. For further details, including source codes, figures, and datasets, please refer to our project’s GitHub repository: [https://github.com/FFarhangian/Fake-news-detection-Comparative-Study].",
    "date": "2023",
    "link": "https://doi.org/10.1016/j.inffus.2023.102140",
    "revista": "Information Fusion",
    "introduction": "It is incontrovertible that we live in an age where social networks play a significant role. Thanks to social networks, many people in modern societies have rapid access to information and news. This easy and free access to social media has led many people to use this media as a source of news [1]. According to the survey of adults in the United States on social media news consumption in 2021, approximately half of the adults in the United States receive news on social media, especially on Twitter and Facebook. Social media prepare a space for individuals to share their opinions or send comments about news or events quickly and without physical contact.\n\nOn the one hand, social networks have led many news agencies to focus more on emerging social networks such as Twitter and Facebook than traditionally publishing news, such as newspapers and magazines [2], [3], [4], [5], [6], [7]. On the other hand, these media have paved the way for profiteers who seek to spread fake news with political and economic motives [1], [5], [6], [7], [8]. Fake news is most of the time related to influential social, political, and economic events such as the US presidential election [9], [10], the COVID-19 pandemic [11], Brexit [12], Syrian civil war [13], and Russia–Ukraine war [14].\n\nAn efficient mechanism to detect fake news and prevent the rapid spread of fake news in society has become one of the most critical challenges in advanced societies [1], [7], [8] because the dissemination of fake news can have devastating effects on society from social, political, and economic aspects. Statistics show that the human ability to distinguish fake news from actual news is almost like tossing coins [8], [15]. Therefore, setting up an accurate and reliable automated system to detect fake news is very important these days [1], [8].\n\nAutomatic Fake news detection is one of the emerging research areas in machine learning and artificial intelligence. The increased number of scientific articles published in the last years in this field compared to previous years is quite evident. In fact, after the 2016 US presidential election and because of its impact on the US election, many scholars turned their attention to fake news detection methods. Fig. 1 shows the increase in Fake news detection publications across the years." 
  },

{
  "title": "Exploring Automatic Hate Speech Detection on Social Media: A Focus on Content-Based Analysis",
  "authors": "Francimaria R. S. Nascimento, George D. C. Cavalcanti, Márjory Da Costa-Abreu",
  "id": 3,
  "text": "Hate speech is a challenging problem, and its dissemination can cause potential harm to individuals and society by creating a sense of general unwelcoming to the marginalized groups, which usually are targeted. Therefore, it is essential to understand this issue and which techniques are useful for automatic detection. This paper presents a survey on automatic hate speech detection on social media, providing a structured overview of theoretical aspects and practical resources. Thus, we review different definitions of the term “hate speech” from social network platforms and the scientific community. We also present an overview of the methodologies used for hate speech detection, and we describe the main approaches currently explored in this context, including popular features, datasets, and algorithms. Furthermore, we discuss some challenges and opportunities for better solving this issue.",
  "date": "2023",
  "link": "https://doi.org/10.1177/21582440231181311",
  "revista": "SAGE Open",
  "introduction": "Social media platforms allow users to publish content about different subjects quickly and easily. Easy content dissemination and anonymity on social media platforms can increase the published harmful content. Different information types can intentionally or unintentionally harm (Giachanou & Rosso, 2020), including misinformation, disinformation, and mal-information. Misinformation (Aswani et al., 2019; Kar & Aswani, 2021), often defined as satirical, is incorrect or fictional information created and spread, disregarding the proper intention. Disinformation (Nasir et al., 2021), for example, fake news is deliberately created to mislead the target users. Mal-information (Davidson et al., 2017; Giachanou & Rosso, 2020), for example, hate speech is created to incite or cause harm. In this survey, we particularly investigate the hate speech detection task.\n\nHate speech is a challenging problem that demonstrates a clear intention to incite harm or promote hatred against others. This issue is considered a worldwide problem faced by many countries and organizations. With the growth of online social media, millions of users can spread much information every second, and the problem has become quite significant. There is a general understanding that when a person feels physically safe, the person’s speech tends to be more aggressive (Watanabe et al., 2018). Moreover, there is a real movement from hate groups to recruit people to create and diffuse hate speech messages (Del Vigna et al., 2017).\n\nThe easy spread of hate speech on online platforms is a serious concern for our society, considering that the dissemination of hate speech can cause potential harm to individual victims and society, for example, raising hostility between groups (Miškolci et al., 2020; Teh et al., 2018). Particularly, repetitive exposure to hate speech can lead to desensitization to this form of violence, thus lowering the victims’ evaluations and increasing the bias against the target groups (Mathew et al., 2019).\n\nSocial media platforms, such as Facebook, Twitter, and YouTube, have claimed they have intended to solve this problem, which they present in policies on hate behavior and attempts to combat hate speech (Facebook, 2020; Twitter, 2020; YouTube, 2020). Much of this content moderation currently requires manual review of questionable documents (Waseem & Hovy, 2016). However, the speed with which such messages are transmitted (shared) makes manual control over message content labor-intensive, time-consuming, expensive, and not scalable (Cao et al., 2020; Zhang et al., 2018).\n\nFurthermore, the hate speech detection task suffers from several weaknesses related to specific nuances of this subject and the complexity of this classification task (Poletto et al., 2021). A relevant issue consists of clearly defining hate speech to understand the problem better and avoid strong subjective interpretations. As we will present in this survey, several disciplines have different definitions for the term “hate speech,” which are complementary.\n\nAll the listed issues and limitations of the manual approaches have motivated considerable research. This survey also aims to provide an overview of better aspects of the problem, such as its definition, different features used in this problem, datasets, and methods. Furthermore, we highlight challenges and draw future work directions, obtaining a theoretical starting ground for new scientists on the topic.\n\nUnderstanding the better aspects of hate speech detection is relevant to dealing with this issue. As a general basis for this area, we found some surveys proposed in this field exploring different questions. In Schmidt and Wiegand (2017) and Fortuna and Nunes (2019), the researchers also survey critical tasks employed for hate speech detection. Nevertheless, it is relevant to note that this field has received increasing attention from the scientific community, and different resources included in the present survey had not been released when these surveys were published or at least when the researchers performed the search. Other works have focused on survey-specific characteristics of hate speech detection, such as multilingual corpus (Al-Hassan & Al-Dossari, 2019), annotated corpora (Poletto et al., 2021), and hate speech on the social media platform Twitter (Ayo et al., 2020).\n\nThis contribution aims to complement these works and present a critical analysis of theoretical aspects and practical resources since this field has constantly grown. (i) We overview a general methodology for hate speech detection on social media, focusing on textual data. (ii) Besides, we present a comprehensive overview of recent resources from different social media and languages, such as the datasets, features used, and algorithms. (iii) We describe the advantages and limitations of several feature extraction techniques currently used in the literature. (iv) We point out different open challenges and opportunities in this field.\n\nThis paper is organized as follows: We first present an analysis of different definitions for the term “hate speech” based on several sources; Then, we explain the methodology used to select the works for this review; Next, we discuss a general methodology for hate speech detection; Then an overview of the related datasets; After, we summarize several feature extraction approaches and present the advantages and limitations of the features explored; Then, we discuss several classification methods used in the literature; Furthermore, we present different challenges highlighted in the literature and opportunities in this field; finally, we conclude this survey with the final remarks."
},
{
  "title": "Unintended bias evaluation: An analysis of hate speech detection and gender bias mitigation on social media using ensemble learning",
  "authors": "Francimaria R.S. Nascimento, George D.C. Cavalcanti, Márjory Da Costa-Abreu",
  "id": 4,
  "text": "Hate speech on online social media platforms is now at a level that has been considered a serious concern by governments, media outlets, and scientists, especially because it is easily spread, promoting harm to individuals and society, and made it virtually impossible to tackle with using just human analysis. Automatic approaches using machine learning and natural language processing are helpful for detection. For such applications, amongst several different approaches, it is essential to investigate the systems’ robustness to deal with biases towards identity terms (gender, race, religion, for example). In this work, we analyse gender bias in different datasets and proposed a ensemble learning approach based on different feature spaces for hate speech detection with the aim that the model can learn from different abstractions of the problem, namely unintended bias evaluation metrics. We have used nine different feature spaces to train the pool of classifiers and evaluated our approach on a publicly available corpus, and our results demonstrate its effectiveness compared to state-of-the-art solutions.",
  "date": "2022",
  "link": "https://doi.org/10.1016/j.eswa.2022.117032",
  "revista": "Expert Systems with Applications",
  "introduction": "The popularisation of social media platforms has driven the exponential growth of the number of textual contents, making manual moderation of such content unsustainable (Cao et al., 2020). In particular, social media platforms allow users to express themselves freely, giving them a false sense of ‘no man’s land’ and promoting a fertile ground for hate speech cases and offensive language usage. Despite its scarcity compared to other contents, the easy dissemination of abusive content on these platforms can be potentially harmful to target individuals, society, governments, and social media (Miškolci et al., 2020).\n\nHate speech is not a trivial phenomenon due to its subjective nature. Fortuna and Nunes (2018) defined it as “Hate speech is language that attacks or diminishes, that incites violence or hate against groups, based on specific characteristics such as physical appearance, religion, descent, national or ethnic origin, sexual orientation, gender identity or other, and it can occur with different linguistic styles, even in subtle forms or when humour is used”. It should be noted that hate speech is usually expressed against a group or a community and may cause potential harm to individuals and society.\n\nIn this context, sexist hate speech has a large space on online social media, usually used against women (Chiril et al., 2020). This type of speech discriminates or harms against a person or group based on a person’s gender. Sexism often is based on a belief in the superiority of a specific sex or gender. Its dissemination can be potentially harmful, and we cannot underestimate its impact on online social media. As an example, widespread sexist hate speech on social media can disseminate gender stereotypes.\n\nSeveral works have proposed methods to perform automatic hate speech detection on benchmark datasets using Natural Language Processing (NLP) with classic Machine Learning (ML) (Salminen et al., 2020, Senarath and Purohit, 2020, Watanabe et al., 2018) and Deep Learning techniques (Zhang & Luo, 2019). So far, this task has been designed in the majority of cases using classic supervised machine learning approaches using metadata, user-based features, text mining-based features, such as lexical approaches, -grams, bag-of-words, text embedding, sentiment, etc., which require a previous definition of the feature extraction methods employed. Deep learning models have explored these approaches for both feature extraction, and classification (Kapil and Ekbal, 2020, Santosh and Aravind, 2019). However, deep learning models require a significant amount of labelled data to perform well. Ensemble learning also has presented robust results, although few explored in the context of hate speech detection (Agarwal and Chowdary, 2021, Al-Makhadmeh and Tolba, 2019, Pitsilis et al., 2018). Even though different contributions have been dedicated to investigating these contents and presented high classification scores, the datasets and algorithms’ potential biases did not receive attention in these researches.\n\nThe skewed distribution of specific terms in the training data can induce questionable trends for particular statements, and the representation learned by the model cannot generalise well enough for practical use (Badjatiya et al., 2019, Dixon et al., 2018, Park et al., 2018). Hence, the supervised model can give unreasonable high hateful scores to clearly non-hateful text, such as “You are a great woman”. The source of this bias can be associate with the highly frequent use of the word “woman” in hateful comments, which the model overgeneralised and associated this word with hateful comments. Dixon et al. (2018) stated this phenomenon as false positive bias and defined this behaviour of recognition models as unintended bias. In particular, they said: “a model contains unintended bias if it performs better for comments containing some particular identity terms than for comments containing others”.\n\nDespite previous efforts, recent studies have investigated concerns about systems’ robustness and discuss the impact of unintended bias in the dataset (Badjatiya et al., 2019, Dixon et al., 2018, Nozza et al., 2019). Some studies investigated bias regarding sensitive words (e.g., lesbian, gay, bisexual, transgender, trans, and so on) and try to mitigate bias based on balancing the training dataset (Dixon et al., 2018) or using replacement strategies (Badjatiya et al., 2019). Moreover, some works presented evidence of racial and dialect biases in several corpora annotated for toxic content, based on the correlation between words related to African American English dialect (AAE) and toxicity ratings (Mozafari et al., 2020, Sap et al., 2019). Gender stereotypes present in benchmark datasets are also a serious concern, in which a model can perform better with determinate identity terms than comments with others (Park et al., 2018). Therefore, it is essential to consider the bias in the datasets and algorithms for hate speech detection. These biases in datasets or classifiers lead to unfairness against target groups, which the classifiers are usually designed to protect.\n\nIn this work, we proposed an ensemble learning method based on different feature spaces for unintended gender bias mitigation in the context of hate speech detection on online social media. The model combines base classifiers, each trained with a different feature representation. Each feature extraction method captures a different abstraction about the data and can present a different classification performance. Therefore, even though one method of feature extraction might fail due to inconsistencies in the data samples (Sajjad et al., 2019) the system can still achieve a good performance as the system also considers other features. We analyse and mitigate gender bias in the datasets using bias-sensitive words and a replacement strategy to bias mitigation.\n\nWe believe that it will revolutionise the fight against gender-based hate speech if we can automatically detect messages of this nature and therefore deal with gender stereotypes present in the system. Thus, we analyse model biases, particularly gender identities (gender bias) present in hate speech datasets. We also propose an approach based on ensemble learning to classify hate speech on online social media and investigate the impact of gender bias in our ensemble method. Hence, this study aims to answer the following research questions: (1) Does the proposed multi-view stacked classifier combined with template-based mitigation outperform current techniques for hate speech detection in the context of unintended gender bias? (2) Can the bias mitigation method deal with gender biases in datasets without compromising the performance of the ensemble learning model?\n\nIn essence, the main contributions of this research are:\n\n• Evaluation of a multi-view stacked classifier using nine different feature spaces combined with template-based mitigation for hate speech detection and gender bias mitigation.\n\n• We perform our experiments in four real-world datasets in the context of gender bias mitigation.\n\n• We explore the model’s behaviour using three base classifiers while considering the unintended gender bias.\n\nThis work is organised as follows: Section 2 describes related work. Section 3 presents the problem statement, Section 4 describe the proposed methodology, Section 5 present the experimental setup, and Section 6 discusses the results. Section 7 concludes the work with the final remarks."
}
]
